# -*- coding: utf-8 -*-
"""text_classification.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1TDYYIBSlzshr0QjTQxTXkEuezVpAL-LG
"""

import numpy as np
import pandas as pd
import os
import sklearn
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder
from sklearn.feature_extraction.text import CountVectorizer
from sklearn.naive_bayes import MultinomialNB
from tpot import TPOTClassifier
import xgboost
'''
!apt-get update -qq 2>&1 > /dev/null
!apt-get install -y -qq software-properties-common python-software-properties module-init-tools
!add-apt-repository -y ppa:alessandro-strada/ppa 2>&1 > /dev/null
!apt-get update -qq 2>&1 > /dev/null
!apt-get -y install -qq google-drive-ocamlfuse fuse

from google.colab import auth
auth.authenticate_user()


from oauth2client.client import GoogleCredentials
creds = GoogleCredentials.get_application_default()
import getpass
# Work around misordering of STREAM and STDIN in Jupyter.
# https://github.com/jupyter/notebook/issues/3159
prompt = !google-drive-ocamlfuse -headless -id={creds.client_id} -secret={creds.client_secret} < /dev/null 2>&1 | grep URL
vcode = getpass.getpass(prompt[0] + '\n\nEnter verification code: ')
!echo {vcode} | google-drive-ocamlfuse -headless -id={creds.client_id} -secret={creds.client_secret}

# Create a directory and mount Google Drive using that directory.
!mkdir -p drive
!google-drive-ocamlfuse drive

print ('Files in Drive:')
!ls drive/
'''
"""Extracting data"""

all_data = pd.read_excel("all_data.xlsx")
corpus = all_data["Narration"]
vectorizer = CountVectorizer(min_df=0) #min_df = ignore low frequency words, 1 ignore all
narration_vec = vectorizer.fit_transform(corpus).toarray()
#print (narration_vec.shape)
#narration_vec

#mlb = sklearn.preprocessing.MultiLabelBinarizer()
#transaction_vec = mlb.fit_transform(all_data['TransactionType']) #binarize
le = LabelEncoder()
le.fit(all_data['TransactionType'])
transaction_vec = le.transform(all_data['TransactionType'])

nar_train,nar_test,trans_train,trans_test = train_test_split(narration_vec,transaction_vec,test_size=0.33,random_state=1)
#print(transaction_vec.shape)
#print(narration_vec.shape)
#print(nar_train.shape)
#print(nar_test.shape)
#print(trans_train.shape)
#print(trans_test.shape)

tpot = TPOTClassifier(generations=5,population_size=50,verbosity=2)
tpot.fit(nar_train,trans_train)
print(tpot.score(nar_test,trans_test))
tpot.export('pipelines/tpot_run_1_pipeline.py')









